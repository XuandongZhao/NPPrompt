# NPPrompt
[ACL 2023] NPPrompt: Pre-trained Language Models Can be Fully Zero-Shot Learners

## [Paper](https://arxiv.org/abs/2212.06950)

We are currently refining the code to align with state-of-the-art Language Models (LLMs). Stay tuned for updates!

We use the dataset in [OpenPrompt](https://github.com/thunlp/OpenPrompt).

## Download the dataset

```bash
cd datasets
bash download_text_classification.sh
```

## Run the code

```bash
bash example_run.sh
```


## Citation

Please cite our paper if you find NPPrompt useful for your research:

```bibtex
@article{zhao2022pre,
  title={Pre-trained Language Models Can be Fully Zero-Shot Learners},
  author={Zhao, Xuandong and Ouyang, Siqi and Yu, Zhiguo and Wu, Ming and Li, Lei},
  journal={arXiv preprint arXiv:2212.06950},
  year={2022}
}
```
